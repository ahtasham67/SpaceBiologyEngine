{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db4efe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NSAC\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files to process\n",
      "\n",
      "Processing: cellular_biology.pdf\n",
      "  ✓ Loaded 23 pages\n",
      "\n",
      "Processing: exvivo.pdf\n",
      "  ✓ Loaded 19 pages\n",
      "\n",
      "Processing: mice_in_bion.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Processing: microgravity_pelvic_bone.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Processing: stem_cells_in_microgravity.pdf\n",
      "  ✓ Loaded 18 pages\n",
      "\n",
      "Total documents loaded: 90\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdfs = process_all_pdfs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082ecbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b498d726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 90 documents into 484 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Academic Editors: John Lawler and\n",
      "Khaled Kamal\n",
      "Received: 21 February 2025\n",
      "Revised: 21 March 2025\n",
      "Accepted: 25 March 2025\n",
      "Published: 27 March 2025\n",
      "Citation: López Garzón, N.A.;\n",
      "Pinzón-Fernández, M.V .;...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-27T16:39:39+08:00', 'author': 'Nelson Adolfo López Garzón, María Virginia Pinzón-Fernández, Jhan S. Saavedra T., Humberto A. Nati-Castillo, Marlon Arias-Intriago, Camila Salazar-Santoliva and Juan S. Izquierdo-Condoy', 'keywords': 'microgravity; tissue effects; immune system; cardiomyocytes; cancer biology; human health', 'moddate': '2025-03-27T09:43:26+01:00', 'subject': 'Microgravity, defined by minimal gravitational forces, represents a unique environment that profoundly influences biological systems, including human cells. This review examines the effects of microgravity on biological processes and their implications for human health. Microgravity significantly impacts the immune system by disrupting key mechanisms, such as T cell activation, cytokine production, and macrophage differentiation, leading to increased susceptibility to infections. In cancer biology, it promotes the formation of spheroids in cancer stem cells and thyroid cancer cells, which closely mimic in vivo tumor dynamics, providing novel insights for oncology research. Additionally, microgravity enhances tissue regeneration by modulating critical pathways, including Hippo and PI3K-Akt, thereby improving stem cell differentiation into hematopoietic and cardiomyocyte lineages. At the organ level, microgravity induces notable changes in hepatic metabolism, endothelial function, and bone mechanotransduction, contributing to lipid dysregulation, vascular remodeling, and accelerated bone loss. Notably, cardiomyocytes derived from human pluripotent stem cells and cultured under microgravity exhibit enhanced mitochondrial biogenesis, improved calcium handling, and advanced structural maturation, including increased sarcomere length and nuclear eccentricity. These advancements enable the development of functional cardiomyocytes, presenting promising therapeutic opportunities for treating cardiac diseases, such as myocardial infarctions. These findings underscore the dual implications of microgravity for space medicine and terrestrial health. They highlight its potential to drive advances in regenerative therapies, oncology, and immunological interventions. Continued research into the biological effects of microgravity is essential for protecting astronaut health during prolonged space missions and fostering biomedical innovations with transformative applications on Earth.', 'title': 'Microgravity and Cellular Biology: Insights into Cellular Responses and Implications for Human Health', 'source': 'data\\\\cellular_biology.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'cellular_biology.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16b6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f20180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x27e927e81a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self,model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading model {self.model_name}: {e}\")\n",
    "\n",
    "    def generate_embeddings(self,texts:List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not loaded.\")\n",
    "        try:\n",
    "            embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error generating embeddings: {e}\")\n",
    "\n",
    "em = EmbeddingManager()\n",
    "em            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b9b3fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "class VectorStore:\n",
    "    def __init__(self,collection_name:str=\"documents\",persist_directory:str=\"data/vector_db\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"pdf embeddings for rag\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized at {self.persist_directory}\")\n",
    "            print(f\"Collection '{self.collection_name}' is ready.\")\n",
    "            print(f\"Existing Documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i,(doc,emb) in enumerate(zip(documents,embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(emb.tolist())\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents.\")\n",
    "            print(f\"Total Documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76964e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m text = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunks\u001b[49m]\n\u001b[32m      2\u001b[39m embeddings = em.generate_embeddings(text)\n\u001b[32m      3\u001b[39m vectorstore.add_documents(chunks,embeddings)\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "text = [doc.page_content for doc in chunks]\n",
    "embeddings = em.generate_embeddings(text)\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b0e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self,vectorstore,embeddingmanager):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embeddingmanager = embeddingmanager\n",
    "\n",
    "    def retrieve(self,query:str,top_k:int = 5, score_threshold:float = 0.0)->List[Dict[str,Any]]:\n",
    "        qemb = self.embeddingmanager.generate_embeddings([query])[0]\n",
    "        try:\n",
    "            results = self.vectorstore.collection.query(\n",
    "                query_embeddings=[qemb.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            retrieved = []\n",
    "            if results['distance'] and results['distance'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                for i,(doc_id,doc,meta,dist) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    score = 1 - dist\n",
    "                    if score >= score_threshold:\n",
    "                        retrieved.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': doc,\n",
    "                            'metadata': meta,\n",
    "                            'similarity_score': score,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved)} documents for the query.\")\n",
    "            else:\n",
    "                print(\"No documents retrieved.\")\n",
    "            return retrieved\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa49965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model_name=\"gemini-1.5-flash\",temperature=0.1,max_output_tokens=1024,google_api_key=gemini_api_key)\n",
    "\n",
    "def rag_func(query,retriever,llm,top_k=3):\n",
    "    res = retriever.retrieve(query,top_k=top_k)\n",
    "    context = \"\\n\\n\".join(doc['content'] for doc in res) if res else \"\"\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file',doc['metadata'].get('source','unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'preview': doc['content'][:300]+'...' if doc['content'] else ''\n",
    "    } for doc in res]\n",
    "    if not context:\n",
    "        return \"No relevant documents found in the knowledge base.\"\n",
    "    prompt = f\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = llm.invoke(prompt.format(context=context,query=query))\n",
    "    output = {\n",
    "        'answer':response.content,\n",
    "        'sources': sources\n",
    "    }\n",
    "    return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSAC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
