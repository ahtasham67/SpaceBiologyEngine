{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db4efe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arafat219/Desktop/Hackathons/SpaceBiologyEngine/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files to process\n",
      "\n",
      "Processing: mice_in_bion.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Processing: cellular_biology.pdf\n",
      "  ✓ Loaded 23 pages\n",
      "\n",
      "Processing: exvivo.pdf\n",
      "  ✓ Loaded 19 pages\n",
      "\n",
      "Processing: stem_cells_in_microgravity.pdf\n",
      "  ✓ Loaded 18 pages\n",
      "\n",
      "Processing: microgravity_pelvic_bone.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Total documents loaded: 90\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdfs = process_all_pdfs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082ecbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    else:\n",
    "        print(\"No documents to split.\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b498d726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 90 documents into 484 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Mice in Bion-M 1 Space Mission: Training and Selection\n",
      "Alexander Andreev-Andrievskiy1,2*, Anfisa Popova1,2, Richard Boyle3, Jeffrey Alberts4, Boris Shenkman1,\n",
      "Olga Vinogradova1, Oleg Dolgov5, Konstant...\n",
      "Metadata: {'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText 5.0.3 (c) 1T3XT BVBA', 'creator': '3B2 Total Publishing System 7.51n/W', 'creationdate': '2014-07-31T12:20:30+08:00', 'title': 'pone.0104830 1..15', 'moddate': '2014-08-08T01:48:51-07:00', 'source': 'data/mice_in_bion.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'mice_in_bion.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16b6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f20180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x72ba8e152870>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self,model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading model {self.model_name}: {e}\")\n",
    "\n",
    "    def generate_embeddings(self,texts:List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not loaded.\")\n",
    "        try:\n",
    "            embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error generating embeddings: {e}\")\n",
    "\n",
    "em = EmbeddingManager()\n",
    "em            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b9b3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized at data/vector_db\n",
      "Collection 'documents' is ready.\n",
      "Existing Documents in collection: 484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x72babc25e420>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "class VectorStore:\n",
    "    def __init__(self,collection_name:str=\"documents\",persist_directory:str=\"data/vector_db\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"pdf embeddings for rag\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized at {self.persist_directory}\")\n",
    "            print(f\"Collection '{self.collection_name}' is ready.\")\n",
    "            print(f\"Existing Documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i,(doc,emb) in enumerate(zip(documents,embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(emb.tolist())\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents.\")\n",
    "            print(f\"Total Documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76964e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 484 documents to the vector store...\n",
      "Successfully added 484 documents.\n",
      "Total Documents in collection: 968\n"
     ]
    }
   ],
   "source": [
    "text = [doc.page_content for doc in chunks]\n",
    "embeddings = em.generate_embeddings(text)\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6b0e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self,vectorstore,embeddingmanager):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embeddingmanager = embeddingmanager\n",
    "\n",
    "    def retrieve(self,query:str,top_k:int = 5, score_threshold:float = 0.0)->List[Dict[str,Any]]:\n",
    "        qemb = self.embeddingmanager.generate_embeddings([query])[0]\n",
    "        try:\n",
    "            results = self.vectorstore.collection.query(\n",
    "                query_embeddings=[qemb.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            print(f\"Retrieved {len(results['documents'][0])} documents for query '{query}'\")\n",
    "            print(f\"Raw distances: {results['distances'][0][:3] if results['distances'][0] else 'None'}\")\n",
    "\n",
    "            retrieved = []\n",
    "            if results['distances'] and results['distances'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i,(doc_id,doc,meta,dist) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    # ChromaDB uses cosine distance, convert to similarity score\n",
    "                    # For cosine distance: similarity = 1 - distance\n",
    "                    # But ChromaDB distance values can be > 1, so we need to handle this properly\n",
    "                    score = max(0, 1 - dist)  # Ensure score is not negative\n",
    "                    \n",
    "                    print(f\"Doc {i+1}: distance={dist:.4f}, score={score:.4f}\")\n",
    "                    \n",
    "                    if score >= score_threshold:\n",
    "                        retrieved.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': doc,\n",
    "                            'metadata': meta,\n",
    "                            'similarity_score': score,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"Doc {i+1} filtered out (score {score:.4f} < threshold {score_threshold})\")\n",
    "                        \n",
    "                print(f\"Retrieved {len(retrieved)} documents after filtering (threshold: {score_threshold}).\")\n",
    "            else:\n",
    "                print(\"No documents retrieved from vector store.\")\n",
    "            return retrieved\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa49965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759464696.831246  120190 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature=0.1,max_output_tokens=2048,google_api_key=gemini_api_key)\n",
    "\n",
    "def rag_func(query,retriever,llm,top_k=3):\n",
    "    res = retriever.retrieve(query,top_k=top_k)\n",
    "    print(f\"Top {len(res)} retrieved documents:\")\n",
    "    \n",
    "    if res:\n",
    "        # We have retrieved documents\n",
    "        context = \"\\n\\n\".join(doc['content'] for doc in res)\n",
    "        sources = [{\n",
    "            'source': doc['metadata'].get('source_file',doc['metadata'].get('source','unknown')),\n",
    "            'page': doc['metadata'].get('page', 'unknown'),\n",
    "            'preview': doc['content'][:300]+'...' if doc['content'] else ''\n",
    "        } for doc in res]\n",
    "        \n",
    "        # Include context in the prompt\n",
    "        prompt = f\"\"\"Use the following context to answer the question. If the context doesn't contain enough information to answer the question, say so clearly.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "    else:\n",
    "        # No documents retrieved, provide general response\n",
    "        sources = []\n",
    "        prompt = f\"\"\"You are a space biology expert. Answer the following question based on these sources about space biology and related fields. If you're not certain about something, please indicate that.\n",
    "        if the sources are not relevant, use your general knowledge to answer the question, but donot tell the user that you have no sources.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'context_used': len(res) > 0\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd5bfb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing with score_threshold=0.0 ===\n",
      "Retrieved 3 documents for query 'what is the effect of space in eyes?'\n",
      "Raw distances: [1.269940972328186, 1.269940972328186, 1.2830924987792969]\n",
      "Doc 1: distance=1.2699, score=0.0000\n",
      "Doc 2: distance=1.2699, score=0.0000\n",
      "Doc 3: distance=1.2831, score=0.0000\n",
      "Retrieved 3 documents after filtering (threshold: 0.0).\n",
      "Top 3 retrieved documents:\n",
      "Answer:\n",
      " The context mentions that the retina and crystalline lens were among the organs and tissues harvested from microgravity-exposed mice for study, as part of research into the adaptation of main physiological systems, including sensory systems, to microgravity. However, it does not describe the specific effects of space on the eyes.\n",
      "Answer:\n",
      " The context mentions that the retina and crystalline lens were among the organs and tissues harvested from microgravity-exposed mice for study, as part of research into the adaptation of main physiological systems, including sensory systems, to microgravity. However, it does not describe the specific effects of space on the eyes.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the effect of space in eyes?\"\n",
    "retriever = RAGRetriever(vectorstore,em)\n",
    "\n",
    "# Test with no threshold first\n",
    "print(\"=== Testing with score_threshold=0.0 ===\")\n",
    "result = rag_func(prompt,retriever,llm,top_k=3)\n",
    "print(\"Answer:\\n\", result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
